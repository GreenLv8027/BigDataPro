# 一、什么是数据仓库？

## 1.数据仓库概念

### 1.1数据仓库

是为企业的所有的决策提供数据支持的数据战略集合！企业的数据仓库需要汇集企业的各种系统，各种类型的数据，在数据仓库中对数据进行清洗，组合，聚合，拆分，计算等，为企业的其他业务需求提供复合的数据支持！



### 1.2数据的产生途径

- 用户主动产生
  - 用户行为数据，用户在使用产品过程中，与客户端产品交互产生的数据
  - 如页面浏览、点击、评论等等。


例如：HTTP请求中，put或者post方式携带的数据。网页埋点，F12，看request，名字为log的

URL，有编码和解码

- 企业主动收集
  - 电商业务数据，用户在使用电商平台提供的和电商业务相关的功能产生的数据
  - 如业务流程中产生的登陆、订单、用户、商品等相关数据信息。




### 1.3要收集什么样的数据？（待补充）（实时补充）

能为公司决策做贡献的数据。

涉及到业务知识（待补充）（探究）

可以通过ADS层，分析出电商所关心的指标有哪些。



## 2.项目需求分析（待补充）（实时补充）

### 2.1业务需求

可以实现**有计划**，每日/每周的**离线分析**

可以实现**随时的指标分析**

分析目标为电商核心主题指标分析，共100个**报表指标**。



### 2.2数据采集需求

搭建**用户行为数据**采集平台

搭建**业务数据**采集平台

建立有分析纬度的数据仓库



### 2.3集群需求

可以对**集群性能**进行监控



### 2.4数据质量需求

可以对数据质量进行监控，如**元数据管理**、**质量监控**









## 3.技术选型

- WEB端框架
  - Nginx，HTTP服务器软件
  - Springboot，web服务端服务器框架

- 数据采集传输
  - Flume，传输日志数据
  - Kafka，实时领域，高通道低延时
  - sqoop，关系型数据和HDFS
  - DataX


- 数据存储
  - MySQL
  - HDFS
  - Hbase
  - Redis，对数据进行非常快速的实时访问，比HBase更快，也更吃内存，因为数据都放在内存中。容易丢数据。
  - MongoDB，文档型数据库，如JSON，HTML，XML


- 数据计算
  - Tez，对MR进行了很多优化，比MR快30倍
  - Spark，第二代大数据计算引擎，比MR快100倍
  - Flink，第三代大数据计算引擎，实时
  - Storm，实时计算引擎，但是需要额外搭一套集群。


- 即席查询
  - Presto，秒级查询
  - Impala，秒级查询
  - Druid，亚秒级查询
  - Kylin，亚秒级查询
- 数据可视化
  - Echarts，百度开发的
  - Superset，深圳公司用得多
- 任务调度
  - Azkaban，Apache平台使用
  - Oozie，CDH平台使用
- 集群监控
  - Zabbix
- 元数据管理
  - Atlas
- 数据质量监控
  - Griffin






## 4.项目架构

### 4.1项目架构图

![Snipaste_2020-03-26_14-26-24](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-26_14-26-24.png)

**解释：**

业务交互数据：业务流程中产生的登录、订单、用户、商品、支付等相关的数据，通常存储在DB中，包括Mysql、Oracle等。

埋点用户行为数据：用户在使用产品过程中，与客户端产品交互过程中产生的数据，比如页面浏览、点击、停留、评论、点赞、收藏等。

数据应用出口：即席查询、离线指标分析、实时指标分析

数据采集入口：MySQL业务数据，logFile文件

即席查询的数据对接：Druid，可以直接对接Kafka。其他3个即席查询，对接Hive

### 4.2用户行为数据的采集

通常用户行为数据是通过APP(webapp,Android,IOS)端埋点的方式，收集到日志服务器中，以日志的形式进行记录！

**数据采集传输架构：**生产Flume——Kafka组——消费Flume

使用flume将日志文件中的数据采集到kafka，再启动flume从kafka中将数据采集到hdfs!

**设计的目的：** 

①Flume的拓扑结构，聚合。使用kakfa进行削峰平谷。HDFS不用连接多个Flume，只用连接少数的消费Flume，减轻NameNode压力。如果每台日志服务器都启动flume进程**直接**向hdfs写入数据，此时hdfs的压力较大。

②分层解耦，项目后期维护性强。将生产数据端，和数据存储端，通过Kafka分离开。两端的业务不会互相受到影响。

③为数据应用出口保留设计。如实时分析业务场景，未来可以使用SparkStreaming直接消费kafka中的数据进行实时分析。

④Flume的拓扑结构，聚合。HDFS可以设置读写权限。生产Flume服务器，可能没有安装Hadoop的jar包，所以本身不能使用HDFS Sink。需要通过消费Flume服务器，安装了Hadoop的jar包，可以使用HDFS Sink。让服务器做更加独立的事情。

​								

### 4.3电商业务数据的采集

用户在使用电商业务期间，产生的数据是存储在mysql中。

数据采集传输架构：业务数据MySQL——Sqoop——HDFS



### 4.4数据的分析

将用户的行为数据和业务数据汇总到**基于hive**创建的数据仓库中，在数据仓库中分层建表分析，共5层。（层数与业务需求有关）

数据计算：将企业关心的数据结果，通过hive计算后，导出到MySQL中。



### 4.5数据的可视化

使用可视化系统从数据结果MySQL中读取计算的结果。

数据展示出口：进行可视化，或被其他的业务系统使用！



### 4.6质量监控

数据仓库的数据仓库，对数据进行检测，比如数据准不准...



### 4.7成本估算

服务器选型，人员需求，集群资源规划

直接成本

自有物理机，需要有机房成本，机器成本等...

购买云主机，省去很多间接成本，如人员成本，但需要考虑数据安全，和竞争信息安全。



间接成本

框架版本收费

人员成本，培训成本





### 4.8框架选型

![Snipaste_2020-03-26_15-45-44](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-26_15-45-44.png)

其他配套工具：

kafka tool，管理kafka主题，查看消息



### 4.9项目架构与项目需求分析的区别（存疑）（未印证）

需求分析在项目架构之前，是以业务部汇总提交的需求材料为基础，经过业务部与数据部的多次讨论，最终得出的业务指标、分析需求等详细的数据架构清单的纲领性文件。

项目架构，在需求分析之后，以满足项目需求分析为目的，提出的技术架构。



## 5.集群规划

### 5.1集群资源规划

![Snipaste_2020-03-26_16-05-05](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-26_16-05-05.png)





### 5.2测试集群进程分配

#### 原则

性能消耗分散在不同的服务器上。

性能消耗，指网络IO、磁盘IO、CPU、内存

不同进程性能消耗不同，如Datanode更多的是消耗磁盘IO。而HRegionserver，更多的是消耗网络IO。Namenode更消耗内存



#### 每个组件的进程特点

- Hadoop，HDFS的NN、YARN的RM很占用内存，需要分开配置。
- MySQL，可以不和Hive、Sqoop，放在一个机器中，只要它们能通过网络IO访问到MySQL就可以。
- Hive，元数据存储在MySQL，需要做一个HA
  - 元数据存储的MySQL部署在除Hive外的服务器上，如103、104
- Flume，放在采集日志的机器，和消费数据的机器。

- Azkaban，调用Hive，需要放在和Hive同一个机器上。

- Hbase，HRegionServer很耗内存
  - HRegionServer一定要部署在HDFS的NameNode的服务器上
    - 否则RegionServer在管理节点时，需要跨节点管理，产生网络IO。
    - 在存储StoreFile时，HDFS会选择在当前RegionServer节点上存一份数据，有利于数据本地化，减少网络IO
- Superset、等其他2个可视化组件，放在哪个机器都可以。




#### 测试集群服务器规划

| 服务名称           | 子服务                   | 服务器  hadoop102 | 服务器  hadoop103 | 服务器  hadoop104 |
| -------------- | --------------------- | -------------- | -------------- | -------------- |
| HDFS           | NameNode              | √              |                |                |
|                | DataNode              | √              | √              | √              |
|                | SecondaryNameNode     |                |                | √              |
| MapReduce      | mrjobhistory          | √              |                |                |
| Yarn           | NodeManager           | √              | √              | √              |
|                | Resourcemanager       |                | √              |                |
| Zookeeper      | Zookeeper Server      | √              | √              | √              |
| Flume(采集日志)    | Flume                 | √              | √              |                |
| Kafka          | Kafka                 | √              | √              | √              |
| Flume（消费Kafka） | Flume                 |                |                | √              |
| Hive           | Hive                  | √              |                |                |
| MySQL          | MySQL                 | √              |                |                |
| Sqoop          | Sqoop                 | √              |                |                |
| Presto         | Coordinator           | √              |                |                |
|                | Worker                |                | √              | √              |
| Azkaban        | AzkabanWebServer      | √              |                |                |
|                | AzkabanExecutorServer | √              |                |                |
| Druid          | Druid                 | √              | √              | √              |
| Kylin          |                       | √              |                |                |
| Hbase          | HMaster               | √              |                |                |
|                | HRegionServer         | √              | √              | √              |
| Superset       |                       | √（实际我放在了104）   |                |                |
| Atlas          |                       | √              |                |                |
| Solr           | Jar                   | √              | √              | √              |
| Griffin        |                       | √              |                |                |
| 服务数总计          |                       | 19             | 9              | 9              |



# 二、数据生成

## 1.用户行为日志数据介绍

### 1.1埋点数据

用户手机的信息，事件信息。Json格式。

只有用户发生了指定的行为，触发了埋点程序，才会记录用户的行为！

```json
埋点数据格式：  { "ap":"数据的来源的设备",

						"cm":{"公共字段属性名":"公共字段属性值" ,.... },

						"et":[{

									"ett":事件发生的时间,

									"en":事件的名称

									"kv":{ 事件的各个属性 }



								},{},....]

					 }

```

埋点数据在发送到服务器后，还会拼接上发送的时间戳。

服务端收到的埋点数据格式：**时间戳 | ｛埋点数据｝**



### 1.2启动日志数据

用户在打开应用的APP，或第一次访问应用时，产生的日志信息称为启动日志.

作用：很重要，可以用来分析用户活跃度。

用户进入的方式：如推送广告进入，图标进入，锁频广告进入等等。

**en**，为start，标记为启动日志数据。

```json
启动日志格式： { 公共字段，启动日志字段， "en":"start"   }
```



### 1.3数据从哪来？

业务和数仓，如果一起开发，没有数据，那么就需要自己造数据。

如果业务先开发，数仓后开发，可以直接向业务要数据。



为什么用fastjson，因为号称是最快的。



## --造数据--

## 补充：Json介绍

### 补1. JSON 值组成：

数字（整数或浮点数）

字符串（在双引号中）

逻辑值（true 或 false）

数组（在方括号中）

对象（在花括号中）

null





## 2.FastJson的使用

### 2.1 JSON

#### 2.1.1 JSON Object

JSON Object转为JSON字符串后，格式如下：

```json
{"id":1001,"name":"zhangsan","age":20,"friends",[{},{}]}
```

#### 2.1.2 JSON Array

JSON Array 转为 JSON字符串后，格式如下：

```json
[{"id":1001,"name":"zhangsan","age":20},
{"id":1001,"name":"zhangsan","age":20},
{"id":1001,"name":"zhangsan","age":20}]
```

### 2.2 FastJson中常用的API

#### 2.2.1 JSON

JSON类是最常用的API！常用方法：

- JSON.toJSONString(Object): 将Java对象转为JSON字符串！

- JSON.parseObject(String, Class): 将JSON字符串，解析为指定的java类型！


#### 2.2.2 JSONObject

JSONObject就代表一个JSON Object对象！本质是使用Map实现！

- 通过JSONObject.get(属性名)，可以获取JSONObject中的某个属性对应的值！

- 通过JSONObject.put(属性名,属性值)，可以向JSONObject中添加某个属性及对应的值！


#### 2.2.3 JSONArray

JSONArray就代表一个JSON Array（JSON数组）对象，本质使用List实现！

- 通过JSONArray.get(index)获取指定位置的参数！

- 通过JSONArray.add(Object e)添加参数！




注意：只有数组或者json数组才可以使用length方法。json对象使用length方法无效。

## 3.Logback

### 3.1 Logback和Log4j的对比

- lo4j比logback出现的早，作者是同一个人！logback是log4j的升级版！logback比log4j速度更快，性能更强，更加稳定！提供了对SLF4J的实现，可以和log4j进行无缝切换！功能更加丰富
- 区别：logback使用.xml文件作为配置文件，log4j使用properties文件作为配置文件

### 3.2 Logback的组件

**Logger**: 日志记录器，在Logger上可以定义日志记录的级别，类型等！

**Appender**: 日志追加器，只要定义日志输出的目的地，输出到控制台还是到文件还是到其他的进程等！

通常必须传入name(appender的名称，可以自定义)，class(append的实现类，可以使用系统的实现类或用自己定义的实现类)

**Layout**: 日志的样式！日志会参照layout的格式，进行格式化输出！



logback.xml中，appender，没有定义error，需要去掉。

## 4.JavaBean类

为什么对象参数，不用基本数据类型？

因为创建对象时，若没有给对象参数赋值，基本数据类型会有一个默认值。如int类型，默认值为0，会对采集的数据有影响？（疑问）



## 5.打包插件

### **5.1maven-compiler-plugin插件**

配置项目使用JDK的哪个版本进行编译

如果已经在maven的settings.xml中配置了此参数，可以省略！



### **5.2 两种maven打包插件**

#### 5.2.1maven-assembly-plugin

- 负责打包
- **会**将pom中的依赖导入jar包！
- 如果jar包运行的环境，没有安装maven，也不会把依赖的Jar包加入到jar包的类路径下
- 可以采用maven-assembly-plugin打的包！

```xml
            <plugin>
                <artifactId>maven-assembly-plugin </artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef><!--生成jar包的后缀名-->
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass>com.atguigu.appclient.AppMain</mainClass> <!--主类名-->
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

```





#### 5.2.2maven-jar-plugin(默认)

- 负责打包
- **不会**将pom中的依赖打入到jar包中！
- 如果jar包的运行环境，已经安装了maven，jar包运行时，可以根据pom中的GAV坐标自动去maven仓库中匹配jar包！
- 可以使用maven-jar-plugin打的包！

```xml
      <plugin>
        <artifactId>maven-jar-plugin</artifactId>
        <version>2.4</version>
        <executions>
          <execution>
            <id>default-jar</id> <!--默认的打包插件-->
            <phase>package</phase>
            <goals>
              <goal>jar</goal><!--打包输出文件的类型-->
            </goals>
          </execution>
        </executions>
      </plugin>
```





项目选择：我们的linux环境，没有maven，所以需要带依赖的打包插件maven-assembly-plugin来打包。



### 5.3 IDEA中的操作细节

**Maven项目中，想看完整的pom.xml文件**

![Snipaste_2020-03-26_17-02-12](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-26_17-02-12.png)

- 可以在pom.xml上右键，Maven-show effective POM ...查看。完整的pom.xml会自动补全默认的配置。


- 我们的linux环境，没有maven，所以需要带依赖的打包插件来打包。




**可以设置主类名**

- 主类名设置，可搜索pom.xml，'mainClass'，在build-plugins-plugin下，插件maven-assembly-plugin中的一个参数。
- 设置后，只需要命令`java -jar jar包名`，就可以运行了。
- 若没有设置主类名，则需要命令`java -cp XX,XXjar包名 主类名 参数`。




# 三、测试集群搭建

## 1.虚拟机搭建和环境配置

### 1.1虚拟机搭建步骤

①需要准备三台虚拟机，要求内存越大越好，磁盘上限设置为50G!

​			16G内存，可以考虑分配4，4，4 或3，5，3

​			32G内存，可以考虑8，8，8

​			24G内存，可以考虑6，6，6

克隆虚拟机即可！

②设置虚拟机可以联网

a)设置每台虚拟机使用独立的MAC地址联网

```
vim /etc/udev/rule.d/70-persistent-net.rules
```

b)每台虚拟机有独立的ip

```
vim /etc/sysconfig/network-scripts/ifcfg-eth0
```

c)集群中的每台虚拟机可以相互联通

查看主机名：

```
hostname
```

查看当前的ip:

```
ifconfig
```

验证：

```
ping hadoop102 -c 3
```

d)集群可以连接互联网

```
ping www.baidu.com -c 3
```

e)集群中的虚拟机已经配置了hosts映射信息

```
vim /etc/hosts
```

f)每天机器拥有自己的主机名

```
vim /etc/sysconfig/network
```

g)修改启动级别为3，不会再启动图形化界面，节省内存

```
vim /etc/inittab
```

③为每台机器配置atguigu用户，赋予免密执行的root权限

添加用户:

```
useradd atguigu
```

为atguigu用户设置密码：

```
passwd atguigu
```

赋予免密执行的root权限：

```
vim /etc/sudoers
```

切换到92行：  92+shift+g 或 92+G

在92行，添加如下内容：

```
atguigu        ALL=(ALL)       NOPASSWD: ALL
```

④在/opt目录下，创建software和module两个目录，用于存在安装的软件，将目录的所属主修改为atguigu

```
mkdir /opt/software
mkdir /opt/module
chown atguigu /opt/software /opt/module
```

⑤关闭防火墙的开机自启动

```
chkconfig iptables off
```



⑥使用图形化界面启动

```
init 5
```

如果使用5号级别登录：

可以使用alt+ctrl+F1切换到命令行（3号），可以使用alt+ctrl+F2切换到图形化界面(5号)



### 1.2**解释和注意**

Q1*/etc/inittab，这是什么？*

init进程就是根据/etc/inittab这个文件来在不同的运行级别启动相应的进程或执行相应的操作。



Q2 *sudo免密，要在sudoers文件中，加nopasswd*



Q3 *连接别人的虚拟机*

怎样连接外网的机器？申请共有IP，或者内网穿透 花生壳

怎样连接同一个教室里的机器？NAT模式的虚拟机，可以直接连上。



### 1.3 Maven

Hadoop-Lzo要进行编译，我没有选择项目所提供的jar包，而是安装了maven自己打包。

#### 通过yum安装

```linux
//下载yum的repo，但是这个repo会选择maven3.5.2。而阿里云没有这个版本，只能通过yum官方下载，很慢很慢
wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

yum -y install apache-maven
```

#### 通过WGET下载

```linux
//这个网址，是cnnic，国内的网址，速度快
wget http://mirrors.cnnic.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz

tar -zxvf解压
配置环境变量，即可使用
export MAVEN_HOME=/usr/local/maven3
export PATH=${PATH}:${MAVEN_HOME}/bin
```

### 1.4 配置ssh免密登录

配置atguigu用户和root用户到集群其他机器的ssh免密登录！

先使用atguigu用户执行以下操作：

```
ssh-keygen -t rsa
ssh-copy-id hadoop102
ssh-copy-id hadoop103
ssh-copy-id hadoop104
```

再切换到root用户，重复执行以上命令！



## 2. Zookeeper

### 2.1 配置zk集群

编辑 $ZOOKEEPER_HOME/conf/zoo.cfg文件，添加如下配置

```properties
dataDir=/opt/module/zookeeper/datas
server.102=hadoop102:2888:3888
server.103=hadoop103:2888:3888
server.104=hadoop104:2888:3888
```

分发文件到集群，修改每台机器 /opt/module/zookeeper/datas/myid文件中的id号为当前机器对应的id!



## 3.Hadoop

### 3.1配置Hadoop

规划： hadoop102-NN,hadoop103-RM,hadoop104-2NN,hadoop102-mrjobhistory

编辑core-site.xml

```xml
<!-- 指定HDFS中NameNode的地址 -->
<property>
<name>fs.defaultFS</name>
    <value>hdfs://hadoop102:9000</value>
</property>

<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/module/hadoop-2.7.2/data/tmp</value>
</property>

```

编辑hdfs-site.xml

```xml
<!-- 指定Hadoop辅助名称节点主机配置 -->
<property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>hadoop104:50090</value>
</property>
```

编辑mapred-site.xml

```xml
<!-- 指定MR运行在YARN上 -->
<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
</property>
<!-- 历史服务进程运行的主机名和rpc端口号 -->
<property>
<name>mapreduce.jobhistory.address</name>
<value>hadoop102:10020</value>
</property>
<!-- 历史服务进程运行的主机名和web端口号 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
<!--第三方框架使用yarn计算的日志聚集功能 ,适用于spark和tez-->
<property>    
    <name>yarn.log.server.url</name>         	       <value>http://hadoop102:19888/jobhistory/logs</value>
</property>

```

编辑yarn-site.xml

日志聚集，让yarn可以看到mrjobhistory中的日志

```xml
<!-- reducer获取数据的方式 -->
<property>
 		<name>yarn.nodemanager.aux-services</name>
 		<value>mapreduce_shuffle</value>
</property>

<!-- 指定YARN的ResourceManager的地址 -->
<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop103</value>
</property>

<!-- 日志聚集功能使能 -->
<property>
<name>yarn.log-aggregation-enable</name>
<value>true</value>
</property>

<!-- 日志保留时间设置7天 -->
<property>
<name>yarn.log-aggregation.retain-seconds</name>
<value>604800</value>
</property>

```

配置完成后，分发配置文件到集群！



mapred-env.sh，为什么不用配置$JAVA_HOME？

因为在之前课上的操作中，$JAVA_HOME已经成为了全局变量。

全局变量，可用命令set查看

### 3.2初始化hadoop

1.格式化NameNode，在namenode节点上运行

```
hadoop namenode -format
```

2.在执行群起的机器上，编写$HADOOP_HOME/etc/hadoop/slaves文件！建议在RM(hadoop103)所在的机器，执行群起脚本！

```
hadoop102
hadoop103
hadoop104
```



### 3.3配置hadoop集群支持lzo压缩

#### 3.3.1 hadoop配置lzo

查看当前集群支持哪些压缩方式：

```
hadoop checknative
```

LZO压缩格式，不支持hadoop平台，需要安装hadoop-lzo组件，才可以让hadoop平台使用LZO压缩格式！

将hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/,之后分发到集群！

编辑：core-site.xml

```xml
<property>
<name>io.compression.codecs</name>
<value>
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.BZip2Codec,
org.apache.hadoop.io.compress.SnappyCodec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec
</value>
</property>

<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>

```

分发core-site.xml，重启hadoop集群！



#### 3.3.2 配置Hadoop支持LZO

```
0. 环境准备
maven（下载安装，配置环境变量，修改sitting.xml加阿里云镜像）
gcc-c++
zlib-devel
autoconf
automake
libtool
通过yum安装即可，yum -y install gcc-c++ lzo-devel zlib-devel autoconf automake libtool

1. 下载、安装并编译LZO

wget http://www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz

tar -zxvf lzo-2.10.tar.gz

cd lzo-2.10

./configure -prefix=/usr/local/hadoop/lzo/

make

make install

2. 编译hadoop-lzo源码

2.1 下载hadoop-lzo的源码，下载地址：https://github.com/twitter/hadoop-lzo/archive/master.zip
2.2 解压之后，修改pom.xml
    <hadoop.current.version>2.7.2</hadoop.current.version>
2.3 声明两个临时环境变量
     export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include
     export LIBRARY_PATH=/usr/local/hadoop/lzo/lib 
2.4 编译
    进入hadoop-lzo-master，执行maven编译命令
    mvn package -Dmaven.test.skip=true
2.5 进入target，hadoop-lzo-0.4.21-SNAPSHOT.jar 即编译成功的hadoop-lzo组件
```



### 3.4HDFS的性能测试

#### 3.4.1测试写性能

```
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB
```

测试结果：

```
20/03/27 15:34:21 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
20/03/27 15:34:21 INFO fs.TestDFSIO:            Date & time: Fri Mar 27 15:34:21 CST 2020
20/03/27 15:34:21 INFO fs.TestDFSIO:        Number of files: 10
一共写了1280M数据
20/03/27 15:34:21 INFO fs.TestDFSIO: Total MBytes processed: 1280.0
集群在写文件时的吞吐量为10m/s
20/03/27 15:34:21 INFO fs.TestDFSIO:      Throughput mb/sec: 10.61685591765301
集群每个节点的平均写的速度为 10.7/s
20/03/27 15:34:21 INFO fs.TestDFSIO: Average IO rate mb/sec: 10.7449369430542
集群每个节点写入数据的标准差 为1.2
标准差反映一组数据举例平均值差值的离散程度，标准差越大，越离散
20/03/27 15:34:21 INFO fs.TestDFSIO:  IO rate std deviation: 1.202543289543699
20/03/27 15:34:21 INFO fs.TestDFSIO:     Test exec time sec: 41.937
```

Throughput，吞吐量 mb/sec。根据之前计算的每日数据量100G，来判断集群的吞吐量是否能够满足要求。



##### 提升写性能的方法（待完善）

影响吞吐量的因素：网络IO，磁盘IO

每个文件，都会用一个线程进程进行写入。当一个节点接受的写入线程越多，那么这些线程就会分配磁盘IO。比如因为机架感知，NN节点因为是本地节点，所以会存入每一个写入文件。

**改进方法：**从影响吞吐量的因素入手。

1. 改进网络通信环境；
2. 升级磁盘性能；
3. 集群扩容，增加节点；
4. 自定义机架感知，减轻NN本地节点的写入数据压力。





#### 3.4.2测试读性能

```
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB

```





磁盘平衡，word笔记中有错误。

bin/start-balancer.sh ，是在集群增加节点后，进行节点间的数据均衡。而不是增加磁盘后，进行的节点内的磁盘均衡。



测试后，记得清除缓存，否则机器会变卡。

```
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean
在运行测试后，可用-clean，清理

```











## 4.lzo（疑问）

### 4.1LZO和LZOP的区别

​		①LZOP依赖于LZO，LZOP是LZO的升级版

​		②LZO是原生的LZO压缩格式，不支持切片！

​			LZOP在LZO的基础上允许我们使用一个为压缩文件创建索引的程序，为每个压缩文件生成索引。

​			基于索引，对压缩文件进行切片！

​		如果使用了LZOP但是，没有创建索引，自然整个文件切一片（无法切片）

​		③LZO压缩格式，生成的压缩文件的后缀名为.lzo.deflate

​			LZOP压缩格式，生成的压缩文件的后缀为.lzo

​		④如果MR程序的输入，读取的.lzo.deflate格式的数据，那么可能出现乱码



总结：  在shuffle阶段，可以使用lzo，但是在reduce的输出和map的输入阶段，使用lzop!



怎样批量的创建lzop的索引文件？（疑问）

for循环？





# 四、日志生成

## 1.在运行jar包时，如何取消在控制台输出日志（疑问）

第一种：从源头上解决，修改我们创建的日志生成程序log.jar中的logback的配置文件，不使用console appender。

第二种： 可以指定linux在运行jar包程序时，将程序向控制台写出的内容定向到 /dev/null (黑洞)，而产生的日志存储在/tmp/logs/（日志生成程序定义的路径）

其中，/tmp/*，里面的文件在centOS中，有15天的保存期。



/dev/null，黑洞是什么？（疑问）



## 2.标准输入和标准输出和错误输出

在linux中，有三个常用的IO设备

- 0：代表stdin标准输入。类似Java中的System.in.scan().接收用户在键盘的信息，传递给标注输入设备
- 1：代表stdout标准输出。类似Java中的System.out.print()，将信息输出到标注输出设备！
- 2：代表stderr标准错误。类似Java中的System.err.print(xx),将信息输出到错误输出设备！

默认情况：`命令 > 文件` 等价于 `命令 1>文件`

```
faefafeafca > a.log 等价于  faefafeafca 1> a.log
```

命令faefafeafca的结果中的标准输出信息，定向到a.log



若需要将stdout和stderr都写入同一个日志文件中，可以使用以下命令

```
faefafeafca 1> c.log 2> c.log 等价于  faefafeafca 1> d.log 2>&1
```

区别： `faefafeafca 1> c.log 2> c.log`会打开c.log文件两次！

​	`faefafeafca 1> d.log 2>&1`只会打开c.log文件一次！效率高！**建议使用**！

解释`2>&1`

2>&1，意思是错误输出等同于标准输出。可以把标准输出和错误输出全部导入日志d.log里。
注意：2>&1，中间不能有空格。
注意：2>&1，必须写在最后，否则会失去原有意义。





# 五、采集管道软件

## 1.flume安装

环境要求： JAVA_HOME 变量，如果需要使用HDFSSink，需要有HADOOP_HOME



## 2. Kafka安装

### 2.1配置

环境要求：JAVA_HOME 变量,安装了zookeeper

配置server.properties

```properties
broker.id=103
delete.topic.enable=true
log.dirs=/opt/module/kafka/datas
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/mykafka
```



- 为什么要在server.properties中，zookeeper.connection参数，在配置的节点后，加上zookeeper根目录节点`/mykafka` ？


因为数仓项目中，可能会把kafka玩坏。删除kafka除了删除软件外，还需要删除里面的数据。指定根目录节点，可以方便的删除这些在zookeeper里的数据。



- 集群停止kafka，使用官方的停止脚本，会出现停不了的情况，为什么？


可能是因为，在启动官方停止脚本前，就已经先把zookeeper停了。



### 2.2生产数据压测

压测命令：

```linux
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092

record-size是一条信息有多大，单位是字节。
num-records是总共发送多少条信息。
throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。
```

压测结果：

```
100000 records sent, 59665.871122 records/sec (5.69 MB/sec), 463.76 ms avg latency, 626.00 ms max latency, 519 ms 50th, 608 ms 95th, 622 ms 99th, 626 ms 99.9th.
```

重点查看：读写速度，延迟

​	59665.871122 records/sec (5.69 MB/sec)，如果生产能力不足，可以对主题添加分区，或扩容集群，或设置kafka同时挂载多个磁盘目录，提高磁盘的IO能力！

​	463.76 ms avg latency： 在测试时，延迟是否可以达到公司的业务场景要求！



#### 如何提高生产性能？

读写速度：可增加主题分区数，或者扩容集群，或者设置kafka同时挂载多个磁盘目录，目的是提高磁盘的IO能力。

为什么增加主题分区数，可以提高性能？

当分区数<=broker数时，增加分区数，可以让更多broker来执行任务，提高了执行任务的磁盘IO。

当分区数>broker数时，增加分区数对提高生产性能的作用就没有那么明显了，因为每个broker都承担了这个任务。



### 2.3消费数据压测(有问题，无法获得数据)

压测命令：

```linux
bin/kafka-consumer-perf-test.sh --broker-list hadoop102:2181 --topic test --fetch-size 10000 --messages 200000 --threads 1
```

压测结果:

```
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2020-03-28 15:19:21:653, 2020-03-28 15:19:26:206, 9.4906, 2.0845, 99516, 21857.2370
```

重点查看：

​	MB.sec： 一秒消费多少M数据

​	nMsg.sec： 一秒消费多少数据

#### 如何提高消费性能？

若四个指标（IO，CPU，内存，网络）都不能改变，则可用以下方法

1. 提高消费者的线程数；前提是一个主题有多个可用的分区！
2. 在消费者端配置一些参数，例如offset的异步提交等！

word笔记有需要改进的：命令用的是--zookeeper，这是老版本的，我们需要用--broker-list



# 六、第一层flume采集管道





![Snipaste_2020-03-28_18-35-20](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-28_18-35-20.png)



## 1.目的

​		将数据从日志文件中使用flume采集到kafka集群中!

## 2.选择Agent中的组件

### 2.1 Source

#### 2.1.1 比较

数据源是在日志文件中，读取日志中的数据，可以使用以下source:

- ExecSource,通过运行tail -f 命令监控文件中新写入的数据。不建议使用，可能会丢失数据

- SpoolingDirSource:  通过监控目录中新写入的文件，将文件的内容读取，封装为agent!

  要求目录中的文件，必须是一成不变的！不能出现重名！

  不用！因为日志中的数据是不断被追加的

- TailDirSource: 接近实时第读取文件中新增的内容！有断点续传功能！

最终选择：TailDirSource!



#### 2.1.2TailDirSource

必须配置：

| **type**                          | –    | `TAILDIR`.      |
| --------------------------------- | ---- | --------------- |
| **filegroups**                    | –    | 组名              |
| **filegroups.filegroup.filename** | –    | 一个组中可以配置多个文件的路径 |

可选参数：

|              |                  |
| ------------ | ---------------- |
| positionFile | 存放postionfile的路径 |

使用问题：在使用中，可以会发现log.jar产生的数据，与flume读取到的数据，相比，flume读取的数据的个数会少。因为TailDirSource以换行符来判定一条数据。这个错误，在flume1.8.0中得到了修正（存疑）



### 2.2 Channel

#### 2.2.1 KafkaChannel的介绍

直接选择KafkaChannel!

​		为什么选择Kafkachannel: 因为kafka集群有高可用和副本机制！这样即便agent挂掉，或某个broker宕机，sink也可以立刻从新的leader上继续拉取event!



#### 2.2.2 KafkaChannel适用的场景-3种配置方式

①source + KafkaChannel + sink，KafkaChannel单次作为channel使用

​		source--->Event---->KafkaProducer--->ProducerRecord---->Kafka Topic<------ kafkaConsumer---->ConsumerRecord----->Event----->Sink

​		可以在KafkaChannel中配置生产者和消费者的参数！

②source +(interceptor)+KafkaChannel , source将数据写入到kafka即可！之后在app端自己写消费者消费kafka中的数据

​		source--->Event---->KafkaProducer--->ProducerRecord---->Kafka Topic

​		可以在KafkaChannel中配置生产者！

③KafkaChannel + sink ，使用sink从kafka中将event写入到其他的目的地！

是一种低延时，容错的方式，从kafka发送events到sink，如HDFS，HBase等

​		Kafka Topic<------ kafkaConsumer---->ConsumerRecord----->Event----->Sink

​			可以在KafkaChannel中配置消费者！

第③个，没有source的那一种方式，无法理解（疑问）

（回答）第3个方法，直接从kafka topic中读取数据。

#### 2.2.3 使用

必须配置：

| **type**                    | –    | `org.apache.flume.channel.kafka.KafkaChannel` |
| --------------------------- | ---- | ---------------------------------------- |
| **kafka.bootstrap.servers** | –    | kafka集群的地址                               |

可选配置：

| kafka.topic             | flume-channel | 将event保存到kafka的哪个主题中                     |
| ----------------------- | ------------- | ---------------------------------------- |
| kafka.consumer.group.id | flume         | 如果要启动消费者，消费者的组id                         |
| parseAsFlumeEvent       | true          | 默认为true将flume的event的body和header一起写入到kafka，如果为false，只将body写入到kafka |

为什么**parseAsFlumeEvent**，是个重要的参数？

会影响最后输出的event中，Header的来源。

原因见下图。

![Snipaste_2020-03-28_15-55-31](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-28_15-55-31.png)



#### 2.2.4 消费者和生产者的配置

①channel层面的参数，例如channel的类型或channel的capicity等和之前配置一样，在channel层面配置，

​		例如： a1.channnels.c1.type=xxx

②和kafka集群相关的参数，以及channel如何操控集群的参数，需要在channel的名称后添加kafka.

​		例如： a1.channnels.c1.kafka.bootstrap.servers

③和生产者和消费者相关的属性，需要添加kafka.producer or kafka.consumer前缀

​		例如： a1.channnels.c1.kafka.producer.acks=-1

​			    a1.channnels.c1.kafka.consumer.group.id=atguigu

## 3.编写拦截器

### 3.1 拦截器的作用

作用一：将日志中的数据按照不同的类型分别存储到kafka的不同主题中。TailDirsouce将日志中的每一行数据封装为一个Event，封装后，我们需要调用拦截器对event进行拦截，拦截后根据event的类型，在header中添加属性，指定了当前event应该发送到哪个主题！

作用二：在拦截器中对数据进行ETL操作！过滤掉不合法的数据！



### 3.2 思路

启动日志：  {}

- 验证是否以{}开头和结尾！



埋点事件日志： xxxxx | {}

- 可以按照`|`进行切分，切分后，判断时间戳是否复合要求；
- 时间戳格式要求：
  - 长度复合13位；
  - 都是是数字
- 判断事件日志是否以{}开头和结尾！



​		

### 3.3 拦截器的实现

#### 3.3.1 MyInterceptor

用来实现拦截器逻辑。

```java
package com.atguigu.warehouse.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Created by VULCAN on 2020/3/28
 */
public class MyInterceptor implements Interceptor {

    private String startFlag="\"en\":\"start\"";

    private List<Event> results=new ArrayList<>();

    //初始化
    @Override
    public void initialize() {

    }

    //核心拦截逻辑
    @Override
    public Event intercept(Event event) {

        //获取body
        byte[] body = event.getBody();
        //转换body为string类型
        String bodyStr = new String(body, Charset.forName("GBK"));

        //根据 日志数据的类型，在header中添加要发往的topic名
        Map<String, String> headers = event.getHeaders();

        //声明一个变量判断数据是否合法
        boolean isLegal=true;


        //判断当前的event中的数据，是否是启动日志
        if (bodyStr.contains(startFlag)){

            headers.put("topic","topic_start");

            //进行启动日志的ETL处理
             isLegal = ETLUtil.validStartLog(bodyStr);



        }else{
            //埋点事件日志
            headers.put("topic","topic_event");

            //进行埋点事件日志的ETL处理
            isLegal = ETLUtil.validEventLog(bodyStr);

        }

        //通过标记判断数据是否合法
        if (!isLegal){
            return null;
        }

        return event;
    }

    //拦截，建议调用intercept(Event event)
    @Override
    public List<Event> intercept(List<Event> events) {

        //先清空results
        results.clear();

        for (Event event : events) {

            Event e = intercept(event);

            //判断拦截的数据是否合法
            if (e !=null){

                //将合法的数据放入到集合中
                results.add(e);

            }

        }

        return results;
    }

    //关闭时调用
    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {

        // 返回一个拦截器对象
        @Override
        public Interceptor build() {
            return new MyInterceptor();
        }

        //读取agent配置文件中的参数
        @Override
        public void configure(Context context) {

        }
    }
}

```



#### 3.3.2 ETLUtil

```java
package com.atguigu.warehouse.interceptor;

import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang.math.NumberUtils;

/**
 * Created by VULCAN on 2020/3/28
 */
public class ETLUtil {

    //处理启动日志
    public static  boolean  validStartLog(String str){

        //先判空
        if (StringUtils.isBlank(str)){

            return false;

        }

        //去前后空格
        String trimStr = str.trim();

        //验证是否以{}开头和结尾
        if (trimStr.startsWith("{") && trimStr.endsWith("}")){

            return true;

        }

        return false;

    }

    //处理事件日志
    public static  boolean  validEventLog(String str){

        //先判空
        if (StringUtils.isBlank(str)){

            return false;

        }

        //去前后空格
        String trimStr = str.trim();

        //按照|进行切分
        String[] words = trimStr.split("\\|");

        //判断格式是否残缺
        if (words.length !=2){

            return false;

        }

        //判断时间戳是否合法 a) 长度复合13位 b)都是是数字
        // NumberUtils.isDigits(): 要求字符串中必须全部为数字符号0-9
        // NumberUtils.isNumbers(): 只要是java支持的数字类型即可
        if (words[0].length() !=13 || !NumberUtils.isDigits(words[0]))
        {
            return  false;
        }

        //验证是否以{}开头和结尾
        if (words[1].startsWith("{") && words[1].endsWith("}")){

            return true;

        }

        return false;

    }
}

```



#### 3.3.3 代码解析

**3.3.1中MyInterceptor代码，17行**

```
private String startFlag="\"en\":\"start\"";
```

这里，是`\"`代表双引号。也就是等于`"en":"start"`

正则表达式中，`|`有特殊含义，需要用`\`转义。而`\`在Java中有特殊含义，如`\n`表示换行，所以需要用`\\`，再加1个`\`来转义反斜杠。最终就写成了`\\|`



**3.3.2中ETLUtil，49行**

```
String[] split = trimStr.split ("\\|");
```

为什么需要2个斜杠。因为这里split的参数，是正则表达式。正则表达式中，`|`为特殊符号，需要用`\|`来表示字符`|`。而Java的String中，`\`为转义符号，需要用`\\`来表示`\`。所以综合起来，就是`\\|`，表达用`|`来分割字符。



**3.3.1MyInterceptor，72行**

```java
 //拦截，建议调用intercept(Event event)
    @Override
    public List<Event> intercept(List<Event> events) {
        //先清空results
        results.clear();
        for (Event event : events) {
            Event e = intercept(event);
            //判断拦截的数据是否合法
            if (e !=null){
                //将合法的数据放入到集合中
                results.add(e);
            }
        }
        return results;
    }
```

List<Event>的这个方法中，也可以用remove的方式，来去除掉Event列表中不符合要求的Event。而不用在初始化1个List了。

但是需要注意，在for增强循环中，不能直接在循环迭代的List对象events使用remove方法，会报并发修改错误，可参考错误集。



### 3.4 拦截器测试配置

```properties
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
#^代表以什么开头，$代表以什么结尾，.代表匹配\n除外任意单个字符,+代表多个
a1.sources.r1.filegroups.f1 = /tmp/logs/^app.+.log$
a1.sources.r1.positionFile=/home/atguigu/taildir_position.json

#拦截器的配置
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.warehouse.interceptor.MyInterceptor$Builder
# 配置sink
a1.sinks.k1.type = logger

# 配置channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000

# 绑定和连接组件
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

## 4.采集管道FlumeAgent编写

### 4.1组件

TailDirsouce---->拦截器-----2 个 KafkaChannel（启动日志放一个主题，事件日志放一个主题）



1个source对接多个channel，需要选择channel选择器(默认ReplicatingChannelSelector)！

我们应该使用MultiPlexingChannelSelector,根据Event中header的topic属性的值将event分发到不同的channel！



### 4.2配置

```properties
a1.sources = r1
a1.channels = c1 c2

# 配置source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
#^代表以什么开头，$代表以什么结尾，.代表匹配\n除外任意单个字符,+代表多个
a1.sources.r1.filegroups.f1 = /tmp/logs/^app.+.log$
a1.sources.r1.positionFile=/home/atguigu/taildir_position.json

#拦截器的配置
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.warehouse.interceptor.MyInterceptor$Builder

#配置MultiplexingChannelSelector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_event = c2
a1.sources.r1.selector.mapping.topic_start = c1


# 配置channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic=topic_start
a1.channels.c1.parseAsFlumeEvent=false

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstfrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic=topic_event
a1.channels.c2.parseAsFlumeEvent=false

# 绑定和连接组件
a1.sources.r1.channels = c1 c2
```





# 七、第2层flume

从kafka中读取日志数据，将日志数据使用hdfssink写入到hdfs的不同的目录中！

## 1.选择agent组件

kafka source - file channel - hdfs sink

要配置topic_start和topic_event，2条线。

可以写成2个配置文件，也可以写在1个配置文件使用multiplexing channel selector



为什么不用kafka Channel？

为了多长长见识。笑

kafka Channel和File Channel一样可以实现不丢数据，而且还快。

## 2.KafkaSource

kafka Source，是kafka的消费者。可以在kafka中设置消费者相关参数的配置。

为什么默认取消自动提交offset？（疑问）

为什么可能会造成数据的重复消费？（疑问）

数据的手动提交，可能会造成数据的重复消费。

### 2.1简介

KafkaSource本质是一个可以从kafka集群的主题上消费数据的消费者！如果希望提高消费者速率，可以配置多个KafkaSource，指定多个KafkaSource有相同的组id！



### 2.2必须配置的参数：

| **type**                    | –     | `org.apache.flume.source.kafka.KafkaSource` |
| --------------------------- | ----- | ---------------------------------------- |
| **kafka.bootstrap.servers** | –     | kafka集群的地址                               |
| kafka.consumer.group.id     | flume | 组id                                      |
| **kafka.topics**            | –     | 要消费哪些主题，多个主题使用,间隔                        |
| **kafka.topics.regex**      | –     | 使用正则匹配要消费的主题！有更高的优先级，如果和**kafka.topics**同时存在，会覆盖之前**kafka.topics**的值 |

### 2.3 可选的参数

| useFlumeEventFormat | false | 必须和之前agent的parseAsFlumeEvent的值一致！ |
| ------------------- | ----- | --------------------------------- |
| batchSize           | 1000  | 一次向channel中写入一批event的数量           |
| batchDurationMillis | 1000  | 一次put操作，间隔的最大时间                   |



和kafka消费者相关的参数如何配置：

添加kafka.consumer.前缀，例如：

​	kafka.consumer.security.protocol=PLAINTEXT



### 2.4 注意事项

- KafkaSource默认取消自动提交offset
- kafkaSource有可能会造成数据的重复消费！

## 3.FileChannel

### 3.1FileChannel的简介

​		FileChannel将event存储在文件中！比memorychannel可靠，效率低！

### 3.2 配置

| **type**            | –                                | `file`.                                  |
| ------------------- | -------------------------------- | ---------------------------------------- |
| checkpointDir       | ~/.flume/file-channel/checkpoint | 在filechannel中有checkpoint线程，负责检查文件中哪些event已经被sink消费走了！这个线程在工作时，需要记录一些信息，信息会保存在checkpiont文件中，此目录代表checkpiont文件保存的路径 |
| useDualCheckpoints  | false                            | 是否备份checkpoint目录，如果启用，需要设置backupCheckpointDir |
| backupCheckpointDir | –                                | 备用目录的路径                                  |
| dataDirs            | ~/.flume/file-channel/data       | 数据文件存储的目录，多个目录使用,间隔！如果多个目录对应多块不同的磁盘，可以提升性能！ |
| keep-alive          | 3                                | 允许一次put操作可以花费的总的时间，如果在这个时间内，put操作没有完成，此次put的数据就回滚！ |



## 4.HDFS Sink

fileType，有3种，DataStream，SequenceStream，CompressedStream

DataStream，不支持压缩，所以不能设置codeC

CompressedStream，需要压缩，要设置codeC

### 4.1 简介

- HDFSSink将event写入到hdfs,支持两种文件类型，分别为TEXT |  SEQUENCEFILE，这两种文件类型都支持压缩
- HDFSSink可以配置文件自动滚动，基于时间 | 基于event的数量 | 基于文件的大小
- HDFSSink还可以指定写入到hdfs的路径使用转义序列，如果是基于时间的转义序列，要求event的header中必须有属性timestamp=时间戳，时间戳会自动替换时间的转义序列。如果event的header中没有此属性，可以指定useLocalTimeStamp=true，默认会使用当前服务器所在的时间作为时间戳
- HDFSSink可以指定目录的滚动策略，原理是将event的时间戳向下舍到最高分|秒|时的倍数的时间



### 4.2 常见的配置

```properties
# 配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = logs-
#滚动目录 一分钟滚动一次目录
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 1
a1.sinks.k1.hdfs.roundUnit = minute
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#配置文件滚动
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217700
a1.sinks.k1.hdfs.rollCount = 0

#使用压缩格式存储数据
a1.sinks.k1.hdfs.fileType=CompressedStream 
#指定文件使用LZO压缩
a1.sinks.k1.hdfs.codeC=lzop
```

要注意，这样设置后，HDFS中的文件名，为flume-agent-source当时消费到的时间戳，转化的年月日。这样，原来kafka中的timestamp就用不上了。

会造成1个问题，kafka采集的时间，会和存储在hdfs上的时间不一致。

如3月30日，23:59分发送的消息，经过第一层采集-kafka-第二层采集时，第二层agent的kafkasource，处理数据时，已经到了3月31日，则此时加上的时间戳，是3月31日的，那么原本3月30日的数据，就保存在了3月31日。

解决方法：可以在第2层flume中，设置拦截器，来从数据中读取时间戳，加到header中。

解决方法2：第1层flume中，设置拦截器在header增加timestamp，kafka channel设置parseAsFlumeEvent=true。第2层flume中，kafka source用use FlumeEventFormat=true

## 5.第二层FlumeAgent的编写

kafkaSource+FileChannel+HDFSSink

2个kafkaSource

2个FileChannel

2个HDFSSink

```properties
a1.sources = r1 r2
a1.channels = c1 c2
a1.sinks = k1 k2

# 配置source
a1.sources.r1.type =org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics = topic_start
a1.sources.r1.kafka.consumer.group.id=CG_start
#kafka消费者默认从分区的最后一个位置消费，当前分区中已经有170条数据，如果不配置，只会从170之后消费
#控制kafka消费者从主题的最早的位置消费，此参数只会在一个从未提交过offset的组中生效
a1.sources.r1.kafka.consumer.auto.offset.reset=earliest

a1.sources.r2.type =org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics = topic_event
a1.sources.r2.kafka.consumer.group.id=CG_event
a1.sources.r2.kafka.consumer.auto.offset.reset=earliest



# 配置channel
a1.channels.c1.type = file
a1.channels.c1.dataDirs = /opt/module/flumedata/c1
a1.channels.c1.checkpointDir=/opt/module/flumedata/c1checkpoint
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.backupCheckpointDir=/opt/module/flumedata/c1backupcheckpoint

a1.channels.c2.type = file
a1.channels.c2.dataDirs = /opt/module/flumedata/c2
a1.channels.c2.checkpointDir=/opt/module/flumedata/c2checkpoint
a1.channels.c2.useDualCheckpoints=true
a1.channels.c2.backupCheckpointDir=/opt/module/flumedata/c2backupcheckpoint

# 配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path=/origin_data/gmall/log/topic_start/%Y-%m-%d
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = startlog-
a1.sinks.k1.hdfs.batchSize=1000
#配置文件滚动
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217700
a1.sinks.k1.hdfs.rollCount = 0
#使用压缩格式存储数据
a1.sinks.k1.hdfs.fileType=CompressedStream 
#指定文件使用LZO压缩
a1.sinks.k1.hdfs.codeC=lzop

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path=/origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = eventlog-
a1.sinks.k2.hdfs.batchSize=1000
a1.sinks.k2.hdfs.rollInterval = 30
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType=CompressedStream 
a1.sinks.k2.hdfs.codeC=lzop

# 绑定和连接组件
a1.sources.r1.channels = c1
a1.sources.r2.channels = c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2
```

## 6.调试思路

kafkaSource------>FileChannel-------->HDFSSink

①查看topic_start和topic_event主题中是否有数据

②验证kafkaSource------>FileChannel是否有问题，通过查看消费者的lag延迟判断！如果lag=0,说明已经消费到了数据，如果lag!=0，kafkaSource有问题。

或也可以再写一个测试的agent，使用kafkaSource------>FileChannel-------->LoggerSink。

注意：每次消费到数据后，只会从之前已经提交offset的最新位置继续消费，也可以通过重置offset来实现！

③验证FileChannel-------->HDFSSink，提高日志级别为WARN，通过查看日志进行调试！



## 7.使用kafka channel- hdfs sink

第2层flume，用kafka channel-hdfs sink，会出现一个时间戳问题。因为第1层flume，存储在kafka topic内的数据没有时间戳数据，所以第2层flume的kafka channel获取不到header的时间戳信息，也就无法解决0点时间的隔日问题。怎样解决？

有1个解决方法：第1层flume中，设置拦截器在header增加timestamp，kafka channel设置parseAsFlumeEvent=true。第2层flume中，kafka channel设置parseAsFlumeEvent=true。但是这样做，在kafka topic中的数据，开头都会多出header信息，这会影响其他数据应用场景如spark的应用吗？

会影响spark应用，是问题。会影响第1层的架构。



可以实现：用hdfs sink的本地时间戳，实现数据从kafka topic读出，写入hdfs。

注意本次数仓经过第1层flume后，存储在kafka topic中的数据，是没有添加header信息的。

因为kafka channel中，没有配置parseAsFlumeEvent，没有将header信息放入Event中。



若要实现用第1层flume，source获取到数据的时间戳来保存数据（避免获取数据时间和消费数据时间跨天问题），则

- 第1层flume，kafka channel需要设置parseAsFlumeEvent = true，并且自定义拦截器要将时间戳放在header中。
- 第2层flume，kafka channel也需要设置parseAsFlumeEvent = true

```properties
a1.channels = c1 c2
a1.sinks = k1 k2

a1.channels.c1.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers= hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.kafka.consumer.group.id = CG1_start
a1.channels.c1.kafka.consumer.auto.offset.reset = earliest
a1.channels.c1.parseAsFlumeEvent = false

a1.channels.c2.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers= hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.kafka.consumer.group.id = CG1_event
a1.channels.c2.kafka.consumer.auto.offset.reset = earliest
a1.channels.c2.parseAsFlumeEvent = false

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start02/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = start-
a1.sinks.k1.hdfs.batchSize = 1000
a1.sinks.k1.hdfs.rollInterval = 30
#这里的数字，不能写1024*1024*128这样计算的数，转换不了数字，而必须写一个整数形式的string
a1.sinks.k1.hdfs.rollSize =  134217700
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzo
a1.sinks.k1.hdfs.useLocalTimeStamp = true

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event02/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = event-
a1.sinks.k2.hdfs.batchSize = 1000
a1.sinks.k2.hdfs.rollInterval = 30
#这里的数字，不能写1024*1024*128这样计算的数，转换不了数字，而必须写一个整数形式的string
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType = CompressedStream
a1.sinks.k2.hdfs.codeC = lzo
a1.sinks.k2.hdfs.useLocalTimeStamp = true

a1.sinks.k1.channel =c1
a1.sinks.k2.channel =c2
```



## 8.模拟造数据

遵循的原则： 将当前机器的时间修改为过去的时间，启动采集通道！如果希望造未来时间的数据，将时间向未来同步，造数据，不需要重新集群！

举例： 如果希望造3月29日的数据，3月22日的数据，3月31日的数据，4月15日的数据

​			需要将集群的时间，同步到3月22日之前。启动采集通道！

​			执行dt.sh 2020-03-22, log.sh 

​			再执行dt.sh 2020-03-29, log.sh 

​			再执行dt.sh 2020-03-31, log.sh 



造多日的数据

从最早一日的日期开始，启动集群，然后边修改日期，边执行log.sh造数据。

注意，必须从最早的日期开始，往后修改日期。如果一开始是较晚的日期，然后再把日期改成较早的日期，造数据时会出现问题，报错kafka channel错误，因为put数据的时间超过了30s（默认值），会认为发送数据失败而报错。



时间戳，在flume中，是什么时候添加的？

第2层，在kafka source中添加？（疑问）



while，循环条件，非空即为true

`$?`，获取上一条命令的返回值



函数function，可以写return，也可以不写return。

不写return，默认返回最后一条命令的执行状态，若为执行正确则返回0，若为执行错误则返回非0的数，如127



flume的put，什么情况下会有重复数据？（疑问）

flume，丢数据的情况？（疑问）



taildir source，监视的文件名滚动的情况，会出现重复数据。如滚动中，正在滚动的文件为XXX.tmp，滚动完成后变成XXX，没有了tmp。那么taildir source会认为这是一个新文件，再次读入，造成重复。

怎么解决？（疑问）改源码，或者改写正则表达式



kafka监控，需要开放jmx端口。（疑问）





编码字典表，用来解释代号，如在支付流水表中，有支付渠道字段，01代表支付宝，02代表微信等等



电商业务表结构，岗位：数据建模工程师



# 四、回顾

## 1.和数仓相关的概念

1.什么是数仓

2.你们公司数仓的数据的输入和输出分别是什么？

3.你们公司数仓的每天的数据量有多少？集群规模是什么？用的是哪种平台（阿里云/物理机）？用了哪些版本的框架？

4.写出5个常用的Linux命令

5.awk,sed,cut,sort,wc的参数



## 2.Hadoop相关

5.3.2除了第三个



## 3.Zookeeper相关

①zk基于的paxos协议，集群半数可用，Leader选举机制

②ZK的读写流程

③zk的常见操作 ： get,set,rmr,create,ls 

④zk的监听器原理



## 4.Flume相关

①flume中的组件，source,sink,channel,场景的组件的配置		

​		taildirSource,spoolingDirSouece,hdfssink,memeryChannnel,FileChannel,KafkaChannel

②Flume的put事务和take事务

③Flume ，event的数据流传输过程，需要画图！

​		体现：

​		ChannelSelector

​		SinkProcessor

​		Intecptor

④监控：基于JMX

​		JMXTrans+InfluxDB+Grafana

⑤会不会丢数据？

​		会！memeryChannnel在agent故障时丢数据！

​				使用异步Source，在客户端无法缓存数据，也会丢数据，典型的ExecSource!

⑥会不会数据重复？

​		会！ put时如何重复？

​				take时如何重复？

⑦在使用TailDirSource时，线上的一次故障情况

​		使用TailDirSource时，可能会出现数据重复的情况！

⑧Flume在数据量大的时候，挂掉如何调优？

​		在$FLUME_HOME/conf/flume.env.sh,配置：

```
export JAVA_OPTS="-Xms4000m -Xmx4000m"
```

## 5.kafka相关

1.手画架构图

2.参考word

3.和broker端调优相关的参数



# X.脚本

## 1 xsync脚本

软连接路径，使用cd，会进入到连接的文件路径中。使用cd -P，进入到真实物理路径中。

将本台机器的文件，分发到集群中相同的父目录下！

例如： 将本机的/home/atguigu/a.log 分发到集群所有机器的 /home/atguigu目录下！

```shell
#!/bin/bash
#脚本必须跟一个文件的相对路径或绝对路径
if(($#!=1))
then
        echo 请输入要分发单个文件的路径!
        exit;
fi

#获取分发文件的物理绝对路径
dirpath=$(cd -P `dirname $1` ; pwd)
filename=`basename $1`

echo "要分发的绝对路径是:$dirpath/$filename"

#获取当前用户的名称
username=`whoami`

#分发
for((i=102;i<=104;i++))
do
        echo "--------------------hadoop$i--------------------"
        rsync -rvlt $dirpath/$filename $username@hadoop$i:$dirpath
done

```



## 2 xcall

```shell
#!/bin/bash
#校验是否传入了命令
if(($#==0))
then
        echo 请输入要执行的命令!
        exit;
fi


echo "要执行的命令是:$@"


#使用ssh登录到目标机器执行命令
for((i=102;i<=104;i++))
do
        echo "--------------------hadoop$i--------------------"
        ssh hadoop$i $@
done
```

## 3 hadoop集群的群起脚本

当前使用atguigu用户执行hadoop集群的启动，将脚本放入到/home/atguigu/bin

当前脚本中，会存在一个问题。yarn.sh需要在配置了ResourceManager的机器上运行。若在Namenode机器上运行yarn.sh，则会出现ResouceManger启动后马上挂掉或者启动不了的情况。

```shell
#!/bin/bash
#hadoop集群的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ] || [ $1 = stop ]
then
        $1-dfs.sh
        $1-yarn.sh
        ssh hadoop102 mr-jobhistory-daemon.sh $1 historyserver
else
        echo 请输入单个start或stop参数！
fi

```

## 4 zk集群的群起脚本

```shell
#!/bin/bash
#zk集群的一键启动脚本，只接收单个start或stop或status参数
if(($#!=1))
then
        echo 请输入单个start或stop或status参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ] || [ $1 = stop ] || [ $1 = status ]
then
        xcall zkServer.sh $1
else
        echo 请输入单个start或stop参数或status！
fi
```



## 5 日志生成脚本

```shell
#!/bin/bash
#调用后，在hadoop102和hadoop103运行日志生成程序，在每台机器的/tmp/logs目录下生成日志数据
for i in hadoop102 hadoop103
do
        ssh $i java -jar /home/atguigu/log.jar $1 $2 > /dev/null 2>&1 &
done
```

`ssh $i java -jar /home/atguigu/log.jar $1 $2 > /dev/null 2>&1`代表程序运行期间产生的标准错误的信息和标注输出的信息全部放入黑洞，控制台上将看不到任何信息。



## 6 集群时间同步到指定日期的脚本

```shell
#!/bin/bash
#dt.sh 日期，可以让集群中所有机器的时间同步到此日期
#如果用户没有传入要同步的日期，同步日期到当前的最新时间
if(($#==0))
then
        xcall sudo ntpdate -u ntp1.aliyun.com
        exit;
fi

#dt.sh 日期，可以让集群中所有机器的时间同步到此日期
for((i=102;i<=104;i++))
do
        echo "--------------同步hadoop$i--------------"
        ssh hadoop$i "sudo date -s '$@'"
done
```



## 7 kafka群起脚本

```shell
#!/bin/bash
#kafka集群的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ]
then
        xcall kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
        elif [ $1 = stop ]
                then xcall kafka-server-stop.sh
else
        echo 请输入单个start或stop参数！
fi
```

### 

## 8.第一层采集通道启动脚本

后台运行的flume，怎样找到它的进程在哪？

运行flume，就是运行的java程序，所以找进程，`ps -ef | grep f1.conf | grep -v grep | cut -d `

awk，默认的就是空格分隔符，所以下面的脚本17行，awk省略了 `' ' `。

awk会把多个空格，当成1个空格。后面的单引号，下面脚本17行`awk  '{print \$2}'`中的单引号，是awk格式需要的单引号。

```shell
#!/bin/bash
#第一层采集通道的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi


#对传入的单个参数进行校验，在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
if [ $1 = start ]
then
        cmd="nohup flume-ng agent -c $FLUME_HOME/conf -n a1 -f $FLUME_HOME/myagents/f1.conf -Dflume.root.logge
r=INFO,console > /home/atguigu/f1.log 2>&1 &"
        elif [ $1 = stop ]
        then
                cmd="ps -ef | grep f1.conf | grep -v grep | awk  '{print \$2}' | xargs kill "
else
        echo 请输入单个start或stop参数！
fi

#在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
for i in hadoop102 hadoop103
do
        echo "--------------$i-----------------"
        ssh $i $cmd
done
```



## 9.编写第二层采集通道的启动脚本

```shell
#!/bin/bash
#第二层采集通道的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi


#对传入的单个参数进行校验，在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
if [ $1 = start ]
then
        ssh hadoop104 "nohup flume-ng agent -c $FLUME_HOME/conf -n a1 -f $FLUME_HOME/myagents/f2.conf -D
flume.root.logger=INFO,console > /home/atguigu/f2.log 2>&1 &"
        elif [ $1 = stop ]
        then
                ssh hadoop104 "ps -ef | grep f2.conf | grep -v grep | awk  '{print \$2}' | xargs kill "
else
        echo 请输入单个start或stop参数！
fi

```


## X.X 脚本编写中的问题

### shell命令中的空格问题

①如果shell命令中有空格，可以使用单引号或双引号引起来

②单引号不会识别`$特殊符号，而双引号会识别$符号`，将$解析为变量的引用

③最外层是双引号，内嵌单引号，$等特殊符号依旧可以识别

④最外层是单引号，内嵌双引号，$等特殊符号无法识别



### bash命令，(())与[]

(())，判断语句之间，没有空格，判断相等，需要用==，C语言风格

[]，判断语句之间，需要有空格，判断相等，只需要用=，XX风格



### 时间同步脚本中，为什么要双引号内嵌单引号？

因为`$`@，可能会得到多个参数，而要让多个参数成为一个整体，需要用单引号。但是单引号并不能解析`$`等特殊字符，所以用双引号来引起来。因为双引号中的单引号内的内容，依然可以解析特殊字符。



### ``反引号

将反引号中的内容作为linux命令执行，赋值给一个变量！等价于$()



linux文本三剑客，需要掌握吗？

掌握AWK就可以，需要深入理解，购买资料AWK书籍，如果岗位需求需要。

目前，掌握所学的就可以了。



### nohup

nuhup保证终端启动的进程在终端断开后，依然在后台运行！

nohup，启动的线程，关掉了SSH终端窗口，依然可以运行。



### xargs

可以将|之前的内容读取为参数，传递给后面的命令！

如果要使用|，通常写法  precmd | postcmd，要求postcmd必须能读取precmd命令的输出才可以！

xartgs，配合管道符`|`使用，可以读取管道符前的标准输出，传递给后面的命令。帮助无法读取的命令，在管道符后获取命令参数。



# 产生问题

<hadoop>Hadoop中，在初始化namenode时，发现只有NN节点成功，有data文件夹，其他的2个服务器没有初始化成功，没有data文件夹。



**java.util.ConcurrentModificationException 并发修改异常**

- <flume>在自定义interceptor中，启动flume agent时，出现以下问题。

![Snipaste_2020-03-28_23-24-49](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-28_23-24-49.png)

**原因：**自定义interceptor中有错误，不能再for循环中，将迭代对象进行更改。

**解决方法：**修改代码，创建另外一个列表来接收符合要求的event。



<kafka>群起kafka后，过一段时间就挂掉。

![Snipaste_2020-03-29_00-05-16](E:\itHeiMa\资料\课程截图类\Snipaste_2020-03-29_00-05-16.png)

**原因1：**因为conf中server.properties的broker.id没有设置正确过来。这个需要每个机器都要有自己的id号。

**原因2：**kafka运行时，会在server.properties中，参数log.dir所配置的路径下（例如本次设置的`/opt/module/kafka/data`），记录server的日志，路径中，有一个名为meta.properties的文件，里面也记录了一个broker.id。这个broker.id与server.properties中的broker.id不匹配，从而产生了启动错误。

**原因2的来源：**为什么会发生这个情况？在原因1发生时，启动过了kafka集群。

因为kafka文件是从hadoop102复制过去的，当时并未修改broker.id。所以在启动kafka时，就发生了这个挂掉的问题。找到了原因1，并解决。但是由于启动了一次kafka，所以就生成了这个meta.propeties这个文件，进一步导致了的原因2。

**解决办法1：**设置好broker.id号，不要重复



<flume>使用TailDir Source-自定义过滤器-Kafka Channel，无法获取到数据

**原因：**自定义过滤器中的业务逻辑写错了，if判断条件，原本为true，但却写反了。

**怎样排查自定义过滤器的问题？：**更改flume的管道配置信息，多试试source，channel，sink或者2个选择器，1个过滤器等，是哪一个部分出现了错误。针对出错的地方，在进行排查。